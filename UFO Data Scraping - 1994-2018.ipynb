{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import beautiful soup library\n",
    "from bs4 import BeautifulSoup\n",
    "# import url handling library\n",
    "import urllib3\n",
    "# imports file handler for writing to csv\n",
    "import csv  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets url parameters for extraction\n",
    "\n",
    "headers = {\n",
    "    \"user-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\"}   \n",
    "baseurl = \"http://www.nuforc.org/webreports/ndxe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates url string for web scraping that conforms to the structure of the target web site\n",
    "def parse_url(mon, yr):\n",
    "    if month < 10:\n",
    "        url = baseurl + (str(year) + '0' + str(month) + '.html')\n",
    "    else:\n",
    "        url = baseurl + (str(year) + str(month) + '.html')\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extracts observations from HTML page\n",
    "def collate_observations (page_text):\n",
    "    th = page_text.find_all(\"th\")\n",
    "    td = page_text.find_all(\"td\")\n",
    "    observations = []\n",
    "    templist = []\n",
    " \n",
    "    j=0\n",
    "    while j < len(td):\n",
    "        templist.append(td[j].text)\n",
    "        if len(templist) % len(th) == 0:\n",
    "            observations.append(templist)\n",
    "            templist = []\n",
    "        j += 1\n",
    "    return observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs data to csv file\n",
    "def write_observations (observations):\n",
    "    j=0\n",
    "    with open('UFO_observations_1994_2018_test.csv', 'a',newline='',  encoding='utf-8') as output:\n",
    "        writer = csv.writer(output)\n",
    "        while j < len(observations):\n",
    "            data = observations[j]\n",
    "            try:\n",
    "                writer.writerow(data)\n",
    "            except UnicodeEncodeError:\n",
    "                data = []\n",
    "            j += 1\n",
    "    output.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to print progress bar - moves the cursor back to the beginning of the line\n",
    "def backspace(n):\n",
    "    print('\\r', end='')\n",
    "    \n",
    "#use to print blank spaces for the progress bar\n",
    "def printblanks(n):\n",
    "    print(\" \", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1994............\n",
      "Reading 1995............\n",
      "Reading 1996............\n",
      "Reading 1997............\n",
      "Reading 1998............\n",
      "Reading 1999............\n",
      "Reading 2000............\n",
      "Reading 2001............\n",
      "Reading 2002............\n",
      "Reading 2003............\n",
      "Reading 2004............\n",
      "Reading 2005............\n",
      "Reading 2006............\n",
      "Reading 2007............\n",
      "Reading 2008............\n",
      "Reading 2009............\n",
      "Reading 2010............\n",
      "Reading 2011............\n",
      "Reading 2012............\n",
      "Reading 2013............\n",
      "Reading 2014............\n",
      "Reading 2015............\n",
      "Reading 2016............\n",
      "Reading 2017............\n",
      "Reading 2018............\n",
      "\n",
      "[DONE]\n"
     ]
    }
   ],
   "source": [
    "#iterates through the website by year and month, extracts the obervations from the HTML before outputting it to a csv file\n",
    "http = urllib3.PoolManager()\n",
    "for year in range(1994,1994):\n",
    "    output_string = \"Reading \" + str(year)\n",
    "    print(output_string, end=\"\")\n",
    "    for month in range (1,2):\n",
    "        page = http.request('GET', parse_url(month, year), headers)\n",
    "        output_string = output_string + \".\"\n",
    "        backspace(len(output_string))\n",
    "        if page.status == 404:\n",
    "            print(output_string, end=\"\")\n",
    "            #output_string=\"\"\n",
    "        else:\n",
    "            print(output_string, end=\"\")\n",
    "            #output_string=\"\"\n",
    "            write_observations((collate_observations (BeautifulSoup(page.data, \"html.parser\"))))\n",
    "    backspace(len(output_string))\n",
    "    output_string=\"\"\n",
    "    print(output_string)\n",
    "print('\\n'+\"[DONE]\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
